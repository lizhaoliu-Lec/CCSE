_BASE_: "./Base-RCNN-FPN.yaml"
MODEL:
  # optimized weights to load
  # WEIGHTS: output/mask_rcnn_R_50_FPN_attention_3x_print_stroke_anchor_scale_init_identity_bs16/20210725.013744/model_0234999.pth
  # WEIGHTS: output/mask_rcnn_R_50_FPN_attention_3x_print_stroke_anchor_scale_init_sam_bs16/20210725.014538/model_0244999.pth
  WEIGHTS: output/mask_rcnn_R_50_FPN_attention_3x_print_stroke_anchor_scale_init_nonlocal_bs16/20210725.025139/model_0034999.pth
  MASK_ON: True
  ANCHOR_GENERATOR:
     SIZES: [ [ 16 ], [ 32 ], [ 64 ], [ 128 ], [ 256 ] ]  # One size for each in feature map init
     ASPECT_RATIOS: [ [ 0.5, 1.0, 2.0, 4.0 ] ]  # Three aspect ratios (same for all in feature maps) init
  ROI_HEADS:
    # coco has 80 classes, but we only have 25 stroke classes
    NUM_CLASSES: 25 # default 80
  ROI_MASK_HEAD:
    NAME: "MaskRCNNConvAttentionUpsampleHead"
    NUM_CONV: 4
    POOLER_RESOLUTION: 14
  ATTENTION:
    # NAME: "Identity"
    # NAME: "SAM"
    # SAM:
      # KERNEL_SIZE: 3
    NAME: "NonLocal"
    NON_LOCAL:
      IN_CHANNEL: 256
DATALOADER:
  NUM_WORKERS: 8 # default 4, too slow
INPUT:
  FORMAT: BGR
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 144
  MAX_SIZE_TRAIN: 144
  MIN_SIZE_TEST: 112
  MIN_SIZE_TRAIN: [112, 120]
DATASETS:
  # <========= handwritten dataset ===========>
  TRAIN: ("handwritten_stroke_2021_train",)
  TEST: ("handwritten_stroke_2021_val",)
  # <========= online dataset ===========>
  #  TRAIN: ("chinese_online_stroke_2021_train",)
  #  TEST: ("chinese_online_stroke_2021_val",)
# the following fields are additionally added to init the env, set output log, and register dataset...
GPU_IDS: [ 0 ] # env related fields
# OUTPUT_ID: mask_rcnn_R_50_FPN_attention_3x_print_stroke_anchor_scale_init_identity_bs16_test # output log are other files related fields
# OUTPUT_ID: mask_rcnn_R_50_FPN_attention_3x_print_stroke_anchor_scale_init_sam_bs16_test # output log are other files related fields
OUTPUT_ID: mask_rcnn_R_50_FPN_attention_3x_print_stroke_anchor_scale_init_nonlocal_bs16_test # output log are other files related fields
# dataset registration related fields
DATA_ROOT: /tmp/binary_handwritten_stroke_2021
DATASET_NAME: handwritten_stroke_2021
TRAIN_JSON_NAME: instances_test2021.json
VAL_JSON_NAME: instances_test2021.json
# <========= rgb test dataset ===========>
# TRAIN_IMAGE_DIR: test2021
# VAL_IMAGE_DIR: test2021
# <========= binary test dataset ===========>
TRAIN_IMAGE_DIR: binarytest2021
VAL_IMAGE_DIR: binarytest2021
VIS_DATASET_RESULT: false # whether to visualize the dataset results
# <========= put the path of any images you want to visualize here ===========>
IMAGE_PATHS: # default None, means not visualizing
  - 'resources/MotivationExpV2/chang.jpg'
  - 'resources/MotivationExpV2/chang-simple.jpg'
  - 'resources/MotivationExpV2/chun.jpg'
  - 'resources/MotivationExpV2/chun-simple.jpg'
  - 'resources/MotivationExpV2/jiang.jpg'
  - 'resources/MotivationExpV2/jiang-simple.jpg'
  - 'resources/MotivationExpV2/tong.jpg'
  - 'resources/MotivationExpV2/tong-simple.jpg'
#  - resources/inferenceImage/hard/mian-linmo.jpg
#  - resources/inferenceImage/hard/mian-shouxie.jpg
#  - resources/inferenceImage/hard/shu-linmo.jpg
#  - resources/inferenceImage/hard/shu-shouxie.jpg
#  - resources/inferenceImage/medium/jian-linmo.jpg
#  - resources/inferenceImage/medium/jian-shouxie.jpg
#  - resources/inferenceImage/medium/jian-yinshua.jpg
#  - resources/inferenceImage/medium/wen-linmo.jpg
#  - resources/inferenceImage/medium/wen-shouxie.jpg
#  - resources/inferenceImage/medium/wen-yinshua.jpg
#  - resources/inferenceImage/simple/san-linmo.jpg
#  - resources/inferenceImage/simple/san-shouxie.jpg
#  - resources/inferenceImage/simple/san-yinshua.jpg
#  - resources/inferenceImage/simple/yi-linmo.jpg
#  - resources/inferenceImage/simple/yi-shouxie.jpg
#  - resources/inferenceImage/simple/yi-yinshua.jpg
#  - resources/inferenceImage/whole.jpg
#  - resources/inferenceImage/dou.jpg
#  - resources/inferenceImage/dou1.jpg
#  - resources/inferenceImage/dou2.jpg
#  - resources/inferenceImage/dou_train_test.png